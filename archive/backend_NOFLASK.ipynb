{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt enginnering & Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Ingest Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "load_dotenv()\n",
    "H2O_API_KEY = os.getenv(\"H2O_API_KEY\") \n",
    "from h2ogpte import H2OGPTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_program(data_dir):\n",
    "    client = H2OGPTE(\n",
    "        address='https://h2ogpte.genai.h2o.ai',\n",
    "        api_key= H2O_API_KEY,\n",
    "    )\n",
    "\n",
    "    # Create a new collection\n",
    "    collection_id = client.create_collection(\n",
    "        name='devMeetingAssistant',\n",
    "        description='Test development for meeting assistant (prompt engineering)',\n",
    "    )\n",
    "    with open(data_dir, 'rb') as f:\n",
    "        meeting_data1 = client.upload('Data/fake_sample.txt', f)\n",
    "    client.ingest_uploads(collection_id, [meeting_data1,])\n",
    "    chat_session_id = client.create_chat_session(collection_id)\n",
    "    return client, collection_id\n",
    "\n",
    "def rag_chat(client, chat_session_id, llm_type, main_prompt, system_prompt):\n",
    "    with client.connect(chat_session_id) as session:\n",
    "        answer = session.query(\n",
    "            message=main_prompt,\n",
    "            system_prompt=system_prompt,\n",
    "            rag_config={\n",
    "            \"rag_type\": \"rag\", # https://h2oai.github.io/h2ogpte/getting_started.html#advanced-controls-for-document-q-a\n",
    "            },\n",
    "            llm=llm_type,\n",
    "        )\n",
    "        return answer\n",
    "\n",
    "def extract_json_string(full_string): \n",
    "    \"\"\"Extracts json-like string from LLM response. Outputs str.\"\"\"\n",
    "\n",
    "    start_index = full_string.find('{')  # Find the index of the first '{'\n",
    "    end_index = full_string.rfind('}')   # Find the index of the last '}'\n",
    "    \n",
    "    if start_index != -1 and end_index != -1:\n",
    "        return full_string[start_index:end_index + 1]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def string_like_JSON_to_txt(json_str):\n",
    "    \"\"\"Input: json-like string\n",
    "    output: Email body string\"\"\"\n",
    "    if json_str is None:\n",
    "        return \"Invalid JSON format. Cannot parse the response.\"\n",
    "\n",
    "    # Parse the extracted JSON string\n",
    "    try:\n",
    "        data = json.loads(json_str)\n",
    "    except json.JSONDecodeError:\n",
    "        return \"Error parsing JSON data.\"\n",
    "\n",
    "    # Extract information from the parsed data\n",
    "    agenda = data.get('Agenda', '')\n",
    "    meeting_summary = data.get('Meeting Summary', '')\n",
    "    actionables = data.get('Actionables', [])\n",
    "\n",
    "    # Format the post-meeting email\n",
    "    email_subject = f\"Post-Meeting Summary: {agenda}\"\n",
    "    email_body = f\"Dear Team,\\n\\nHere's a summary of our recent meeting:\\n\\nAgenda: {agenda}\\n\\nMeeting Summary:\\n{meeting_summary}\\n\\nActionables:\\n\"\n",
    "    for action in actionables:\n",
    "        email_body += f\"- {action['Action']} (Assigned to: {action['Assigned']}, Deadline: {action['Deadline']}, Priority: {action['Priority']})\\n\"\n",
    "\n",
    "    email_body += \"\\nPlease let me know if anything needs clarification or if there are additional action items to add.\\n\\nBest regards,\\n[Your Name]\"\n",
    "    return email_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_response(client, chat_session_id, llm_type):\n",
    "    system_prompt = \"\"\"\n",
    "    You are a Meeting Assistant for a large company that wants to maximize meeting efficiency. \n",
    "    You will be given meeting transcript(s) from real project meetings held over zoom.\n",
    "    Follow all the instructions closely or everyone in the team will be fired.\n",
    "    If you do well, you will be rewarded with a $1000 tip.\n",
    "    \"\"\"\n",
    "    main_prompt = \"\"\"\n",
    "    Execute all the following steps: \\\n",
    "    1. Examine the document and identify all names of the attendees, remember these names for the next steps.\n",
    "    2. Identify and extract the main purpose or agenda of the meeting\n",
    "    3. Identify all actionables mentioned during the meeting\n",
    "    4. Extract all the actionables and action plans, identify all details related to it including the deadlines, who it is assigned to, etc.\n",
    "    5. Summarize the entire transcript\n",
    "    6. Present all identified information in the following JSON format for easy parsing:\n",
    "    {\"Agenda\": \"\", \"Meeting Summary\":\"\", \"Actionables\":[{\"Action\":\"\", \"Deadline\":\"\", \"Assigned\":\"\"}, ...]}\n",
    "    DO NOT include anything other than the JSON format output in your response.\n",
    "    \"\"\"\n",
    "    return rag_chat(client, chat_session_id, llm_type, main_prompt, system_prompt)\n",
    "\n",
    "def get_sentiment_response(client, chat_session_id, llm_type, meeting_summary_response):\n",
    "    system_prompt = \"\"\"\n",
    "    You are a Meeting Assistant for a large company that wants to maximize meeting efficiency. \n",
    "    You will be given meeting transcript(s) from real project meetings held over zoom.\n",
    "    Follow all the instructions closely or everyone in the team will be fired.\n",
    "    If you do well, you will be rewarded with a $1000 tip.\n",
    "    \"\"\"\n",
    "    sentiment_prompt = f\"\"\"\n",
    "    Execute all the following steps: \\\n",
    "    1. Examine the contexts of this JSON object: {meeting_summary_response}\n",
    "    2. For each actionable, execute the following steps\n",
    "    a. identify the sentence in the document where it is mentioned\n",
    "    b. Interpret the sentence, based on this context, categorize the priority of this task as \"High\", \"Medium\" or \"Low\"\n",
    "    c. Edit the JSON object, add a column to the dictionary for this action as follows: \"Priority\": \"\"\n",
    "    3. Present all identified information in the following JSON format:\n",
    "    {{\"Agenda\": \"\", \"Meeting Summary\":\"\"}} for the entire meeting\n",
    "    {{\"Actionables\":[{{\"Action\":\"\", \"Deadline\":\"\", \"Assigned\":\"\", \"Priority\":\"\"}}]}} for each action plan\n",
    "    \"\"\"\n",
    "\n",
    "    return rag_chat(client, chat_session_id, llm_type, sentiment_prompt, system_prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LLM_response(data_dir, llm_type):\n",
    "    \"\"\"\n",
    "    Output: json object\n",
    "    \"\"\"\n",
    "    client, collection_id = init_program(data_dir)\n",
    "    chat_session_id = client.create_chat_session(collection_id)\n",
    "    # 1. Summary and action plan\n",
    "    meeting_summary_response = get_summary_response(client, chat_session_id, llm_type)\n",
    "\n",
    "    # 2. Sentiment analysis/Priority allocation\n",
    "    sentiment_reply = get_sentiment_response(client, chat_session_id, llm_type ,meeting_summary_response)\n",
    "\n",
    "    # Convert to email body\n",
    "    return string_like_JSON_to_txt(sentiment_reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"Data/fake_sample.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Server version 1.4.7 doesn't match client version 1.4.0: unexpected errors may occur.\n",
      "Please install the correct version of H2OGPTE with `pip install h2ogpte==1.4.7`.\n",
      "You can enable strict version checking by passing strict_version_check=True.\n",
      "Invalid JSON format. Cannot parse the response.\n"
     ]
    }
   ],
   "source": [
    "email_reply_llama70 = get_LLM_response(data_dir,llm_type='h2oai/h2ogpt-4096-llama2-70b-chat')\n",
    "print(email_reply_llama70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_response(data_dir, llm_type):\n",
    "    client, collection_id = init_program(data_dir)\n",
    "    chat_session_id = client.create_chat_session(collection_id)\n",
    "    # 1. Summary and action plan\n",
    "    meeting_summary_response = get_summary_response(client, chat_session_id, llm_type)\n",
    "    \n",
    "    sentiment_response = get_sentiment_response(client, chat_session_id, llm_type, extract_json_string(meeting_summary_response.content))\n",
    "    print(sentiment_response.content)\n",
    "    return string_like_JSON_to_txt(sentiment_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Server version 1.4.9 doesn't match client version 1.4.0: unexpected errors may occur.\n",
      "Please install the correct version of H2OGPTE with `pip install h2ogpte==1.4.9`.\n",
      "You can enable strict version checking by passing strict_version_check=True.\n",
      "{\n",
      "\"Agenda\": \"Reviewing progress on the project, discussing roadblocks, and planning next steps\",\n",
      "\"Meeting Summary\": \"The meeting discussed the current status of the project, reviewed the design proposal, and finalized the budget allocation. The team also discussed potential roadblocks and found solutions, and scheduled a follow-up discussion. The meeting concluded with a summary of actionables and their deadlines.\",\n",
      "\"Actionables\": [\n",
      "{\n",
      "\"Action\": \"Complete design proposal\",\n",
      "\"Deadline\": \"Wednesday\",\n",
      "\"Assigned\": \"Ryan_Edward\",\n",
      "\"Priority\": \"High\"\n",
      "},\n",
      "{\n",
      "\"Action\": \"Confirm meeting with design specialist\",\n",
      "\"Deadline\": \"Thursday\",\n",
      "\"Assigned\": \"Ben_CH\",\n",
      "\"Priority\": \"Medium\"\n",
      "},\n",
      "{\n",
      "\"Action\": \"Resolve integration issue\",\n",
      "\"Deadline\": \"Friday\",\n",
      "\"Assigned\": \"Chan Yi Ru Micole\",\n",
      "\"Priority\": \"Medium\"\n",
      "},\n",
      "{\n",
      "\"Action\": \"Share progress document\",\n",
      "\"Deadline\": \"After the meeting\",\n",
      "\"Assigned\": \"Ryan_Edward\",\n",
      "\"Priority\": \"Low\"\n",
      "},\n",
      "{\n",
      "\"Action\": \"Send out meeting details\",\n",
      "\"Deadline\": \"Thursday\",\n",
      "\"Assigned\": \"Ben_CH\",\n",
      "\"Priority\": \"Low\"\n",
      "}\n",
      "]\n",
      "}\n",
      "\n",
      "Explanation:\n",
      "\n",
      "1. The agenda and meeting summary are self-explanatory.\n",
      "2. For each actionable, the sentence where it is mentioned is identified, and the priority is interpreted based on the context.\n",
      "* Complete design proposal: \"Ryan, could you give us an update on the tasks you've been working on?\" (High priority)\n",
      "* Confirm meeting with design specialist: \"Ben, please coordinate that.\" (Medium priority)\n",
      "* Resolve integration issue: \"Micole, have you encountered any challenges or need assistance with your tasks?\" (Medium priority)\n",
      "* Share progress document: \"Ryan, could you give us an update on the tasks you've been working on?\" (Low priority)\n",
      "* Send out meeting details: \"Ben, please coordinate that.\" (Low priority)\n",
      "3. The information is presented in the requested JSON format.\n",
      "Dear Team,\n",
      "\n",
      "Here's a summary of our recent meeting:\n",
      "\n",
      "Agenda: Reviewing progress on the project, discussing roadblocks, and planning next steps\n",
      "\n",
      "Meeting Summary:\n",
      "The meeting discussed the current status of the project, reviewed the design proposal, and finalized the budget allocation. The team also discussed potential roadblocks and found solutions, and scheduled a follow-up discussion. The meeting concluded with a summary of actionables and their deadlines.\n",
      "\n",
      "Actionables:\n",
      "- Complete design proposal (Assigned to: Ryan_Edward, Deadline: Wednesday, Priority: High)\n",
      "- Confirm meeting with design specialist (Assigned to: Ben_CH, Deadline: Thursday, Priority: Medium)\n",
      "- Resolve integration issue (Assigned to: Chan Yi Ru Micole, Deadline: Friday, Priority: Medium)\n",
      "- Share progress document (Assigned to: Ryan_Edward, Deadline: After the meeting, Priority: Low)\n",
      "- Send out meeting details (Assigned to: Ben_CH, Deadline: Thursday, Priority: Low)\n",
      "\n",
      "Please let me know if anything needs clarification or if there are additional action items to add.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n"
     ]
    }
   ],
   "source": [
    "test_response = test_response(data_dir, llm_type='h2oai/h2ogpt-4096-llama2-70b-chat')\n",
    "print(test_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Server version 1.4.7 doesn't match client version 1.4.0: unexpected errors may occur.\n",
      "Please install the correct version of H2OGPTE with `pip install h2ogpte==1.4.7`.\n",
      "You can enable strict version checking by passing strict_version_check=True.\n",
      "{\n",
      "\"Agenda\": \"Reviewing progress on the project, discussing roadblocks, and planning next steps\",\n",
      "\"Meeting Summary\": \"The meeting discussed the current status of the project, reviewed the design proposal, and finalized the budget allocation. The team also discussed potential roadblocks and found solutions, and scheduled a follow-up discussion. The meeting concluded with a summary of actionables and their deadlines.\",\n",
      "\"Actionables\": [\n",
      "{\n",
      "\"Action\": \"Complete design proposal\",\n",
      "\"Deadline\": \"Wednesday\",\n",
      "\"Assigned\": \"Ryan_Edward\",\n",
      "\"Priority\": \"High\"\n",
      "},\n",
      "{\n",
      "\"Action\": \"Confirm meeting with design specialist\",\n",
      "\"Deadline\": \"Thursday\",\n",
      "\"Assigned\": \"Ben_CH\",\n",
      "\"Priority\": \"Medium\"\n",
      "},\n",
      "{\n",
      "\"Action\": \"Resolve integration issue\",\n",
      "\"Deadline\": \"Friday\",\n",
      "\"Assigned\": \"Chan Yi Ru Micole\",\n",
      "\"Priority\": \"Medium\"\n",
      "},\n",
      "{\n",
      "\"Action\": \"Share progress document\",\n",
      "\"Deadline\": \"After the meeting\",\n",
      "\"Assigned\": \"Ryan_Edward\",\n",
      "\"Priority\": \"Low\"\n",
      "},\n",
      "{\n",
      "\"Action\": \"Send out meeting details\",\n",
      "\"Deadline\": \"Thursday\",\n",
      "\"Assigned\": \"Ben_CH\",\n",
      "\"Priority\": \"Low\"\n",
      "}\n",
      "]\n",
      "}\n",
      "\n",
      "Explanation:\n",
      "\n",
      "1. The agenda and meeting summary are self-explanatory.\n",
      "2. For each actionable, the sentence where it is mentioned is identified, and the priority is interpreted based on the context.\n",
      "* Complete design proposal: \"Ryan, could you give us an update on the tasks you've been working on?\" (High priority)\n",
      "* Confirm meeting with design specialist: \"Ben, please coordinate that.\" (Medium priority)\n",
      "* Resolve integration issue: \"Micole, have you encountered any challenges or need assistance with your tasks?\" (Medium priority)\n",
      "* Share progress document: \"Ryan, could you give us an update on the tasks you've been working on?\" (Low priority)\n",
      "* Send out meeting details: \"Ben, please coordinate that.\" (Low priority)\n",
      "3. The information is presented in the requested JSON format.\n",
      "Dear Team,\n",
      "\n",
      "Here's a summary of our recent meeting:\n",
      "\n",
      "Agenda: Reviewing progress on the project, discussing roadblocks, and planning next steps\n",
      "\n",
      "Meeting Summary:\n",
      "The meeting discussed the current status of the project, reviewed the design proposal, and finalized the budget allocation. The team also discussed potential roadblocks and found solutions, and scheduled a follow-up discussion. The meeting concluded with a summary of actionables and their deadlines.\n",
      "\n",
      "Actionables:\n",
      "- Complete design proposal (Assigned to: Ryan_Edward, Deadline: Wednesday, Priority: High)\n",
      "- Confirm meeting with design specialist (Assigned to: Ben_CH, Deadline: Thursday, Priority: Medium)\n",
      "- Resolve integration issue (Assigned to: Chan Yi Ru Micole, Deadline: Friday, Priority: Medium)\n",
      "- Share progress document (Assigned to: Ryan_Edward, Deadline: After the meeting, Priority: Low)\n",
      "- Send out meeting details (Assigned to: Ben_CH, Deadline: Thursday, Priority: Low)\n",
      "\n",
      "Please let me know if anything needs clarification or if there are additional action items to add.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n"
     ]
    }
   ],
   "source": [
    "test_response = test_response(data_dir, llm_type='h2oai/h2ogpt-4096-llama2-70b-chat')\n",
    "print(test_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response= \"\"\"{\n",
    "\"Agenda\": \"Reviewing progress on the project, discussing roadblocks, and planning next steps\",\n",
    "\"Meeting Summary\": \"The meeting discussed the current status of the project, including completed tasks and ongoing challenges. The team reviewed the project timeline and identified potential roadblocks, such as ensuring design alignment with industry standards. Action plans were established, including scheduling a meeting with a design specialist and finalizing the budget allocation. The team also discussed the integration issue and testing progress.\"\n",
    "\"Actionables\": [\n",
    "{\n",
    "\"Action\": \"Schedule a meeting with a design specialist\",\n",
    "\"Deadline\": \"This week\",\n",
    "\"Assigned\": \"Ben\"\n",
    "},\n",
    "{\n",
    "\"Action\": \"Finalize budget allocation\",\n",
    "\"Deadline\": \"End of the week\",\n",
    "\"Assigned\": \"Ben\"\n",
    "},\n",
    "{\n",
    "\"Action\": \"Complete design proposal\",\n",
    "\"Deadline\": \"Wednesday\",\n",
    "\"Assigned\": \"Ryan\"\n",
    "},\n",
    "{\n",
    "\"Action\": \"Testing and resolve integration issue\",\n",
    "\"Deadline\": \"Friday\",\n",
    "\"Assigned\": \"Micole\"\n",
    "}\n",
    "]\n",
    "}\n",
    "\n",
    "The attendees' names are:\n",
    "\n",
    "* Eugene_YJ\n",
    "* Ryan_Edward\n",
    "* Ben_CH\n",
    "* Chan Yi Ru Micole\"\"\"\n",
    "json_str = json.loads(extract_json_string(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing JSON data.\n"
     ]
    }
   ],
   "source": [
    "print(meet_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"Agenda\": \"Reviewing progress on the project, discussing roadblocks, and planning next steps\",\n",
      "\"Meeting Summary\": \"The meeting discussed the current status of the project, including completed tasks and ongoing challenges. The team reviewed the project timeline and identified potential roadblocks, such as ensuring design alignment with industry standards. Action plans were established, including scheduling a meeting with a design specialist and finalizing the budget allocation. The team also discussed the integration issue and testing progress.\"\n",
      "\"Actionables\": [\n",
      "{\n",
      "\"Action\": \"Schedule a meeting with a design specialist\",\n",
      "\"Deadline\": \"This week\",\n",
      "\"Assigned\": \"Ben\"\n",
      "},\n",
      "{\n",
      "\"Action\": \"Finalize budget allocation\",\n",
      "\"Deadline\": \"End of the week\",\n",
      "\"Assigned\": \"Ben\"\n",
      "},\n",
      "{\n",
      "\"Action\": \"Complete design proposal\",\n",
      "\"Deadline\": \"Wednesday\",\n",
      "\"Assigned\": \"Ryan\"\n",
      "},\n",
      "{\n",
      "\"Action\": \"Testing and resolve integration issue\",\n",
      "\"Deadline\": \"Friday\",\n",
      "\"Assigned\": \"Micole\"\n",
      "}\n",
      "]\n",
      "}\n",
      "\n",
      "The attendees were:\n",
      "Eugene_YJ\n",
      "Ryan_Edward\n",
      "Ben_CH\n",
      "Chan Yi Ru Micole\n"
     ]
    }
   ],
   "source": [
    "meet_response = test_response(data_dir, llm_type='h2oai/h2ogpt-4096-llama2-70b-chat')\n",
    "print(meet_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"{'base_model': 'mistralai/Mixtral-8x7B-Instruct-v0.1', 'prompt_type': 'mixtral', 'prompt_dict': {'promptA': '<<SYS>>\\nYou are an AI that follows instructions extremely well and as helpful as possible.\\n<</SYS>>\\n\\n', 'promptB': '<<SYS>>\\nYou are an AI that follows instructions extremely well and as helpful as possible.\\n<</SYS>>\\n\\n', 'PreInstruct': '<s> [INST] ', 'PreInput': None, 'PreResponse': '[/INST]', 'terminate_response': ['[INST]', '</s>'], 'chat_sep': ' ', 'chat_turn_sep': '</s> ', 'humanstr': '[INST]', 'botstr': '[/INST]', 'generates_leading_space': False, 'system_prompt': 'You are an AI that follows instructions extremely well and as helpful as possible.', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 32718, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
    "{'base_model': 'h2oai/h2ogpt-4096-llama2-70b-chat', 'prompt_type': 'llama2', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': \"<s>[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\n\", 'PreInput': None, 'PreResponse': '[/INST]', 'terminate_response': ['[INST]', '</s>'], 'chat_sep': ' ', 'chat_turn_sep': ' </s>', 'humanstr': '[INST]', 'botstr': '[/INST]', 'generates_leading_space': False, 'system_prompt': \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\", 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 4046, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
    "{'base_model': 'h2oai/h2ogpt-4096-llama2-13b-chat', 'prompt_type': 'llama2', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': \"<s>[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\n\", 'PreInput': None, 'PreResponse': '[/INST]', 'terminate_response': ['[INST]', '</s>'], 'chat_sep': ' ', 'chat_turn_sep': ' </s>', 'humanstr': '[INST]', 'botstr': '[/INST]', 'generates_leading_space': False, 'system_prompt': \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\", 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 4046, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
    "{'base_model': 'h2oai/h2ogpt-32k-codellama-34b-instruct', 'prompt_type': 'llama2', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': \"<s>[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\n\", 'PreInput': None, 'PreResponse': '[/INST]', 'terminate_response': ['[INST]', '</s>'], 'chat_sep': ' ', 'chat_turn_sep': ' </s>', 'humanstr': '[INST]', 'botstr': '[/INST]', 'generates_leading_space': False, 'system_prompt': \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\", 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 32718, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
    "{'base_model': 'HuggingFaceH4/zephyr-7b-beta', 'prompt_type': 'zephyr', 'prompt_dict': {'promptA': '<|system|>\\nYou are an AI that follows instructions extremely well and as helpful as possible.</s>\\n', 'promptB': '<|system|>\\nYou are an AI that follows instructions extremely well and as helpful as possible.</s>\\n', 'PreInstruct': '<|user|>\\n', 'PreInput': None, 'PreResponse': '</s>\\n<|assistant|>\\n', 'terminate_response': ['<|assistant|>', '</s>'], 'chat_sep': '', 'chat_turn_sep': '</s>\\n', 'humanstr': '<|user|>', 'botstr': '<|assistant|>', 'generates_leading_space': False, 'system_prompt': 'You are an AI that follows instructions extremely well and as helpful as possible.', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 32718, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
    "{'base_model': 'NousResearch/Nous-Capybara-34B', 'prompt_type': 'vicuna11', 'prompt_dict': {'promptA': \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. \", 'promptB': \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. \", 'PreInstruct': 'USER: ', 'PreInput': None, 'PreResponse': 'ASSISTANT:', 'terminate_response': ['ASSISTANT:', '</s>'], 'chat_sep': ' ', 'chat_turn_sep': '</s>', 'humanstr': 'USER: ', 'botstr': 'ASSISTANT:', 'generates_leading_space': False, 'system_prompt': \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\", 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 199950, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
    "{'base_model': 'claude-2.1', 'prompt_type': 'anthropic', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '\\n', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 199950, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
    "{'base_model': 'gpt-3.5-turbo-0613', 'prompt_type': 'openai_chat', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '\\n', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 4046, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
    "{'base_model': 'gpt-3.5-turbo-16k-0613', 'prompt_type': 'openai_chat', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '\\n', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 16335, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
    "{'base_model': 'gpt-35-turbo-1106', 'prompt_type': 'openai_chat', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '\\n', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 16335, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
    "{'base_model': 'gpt-4-1106-preview', 'prompt_type': 'openai_chat', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '\\n', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 127950, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
    "{'base_model': 'gemini-pro', 'prompt_type': 'google', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '\\n', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': 'I am a helpful assistant.  I will accurately answer all your questions.', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 32718, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
    "{'base_model': 'mistral-small-latest', 'prompt_type': 'mistralai', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '\\n', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 32718, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
    "{'base_model': 'mistral-large-latest', 'prompt_type': 'mistralai', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '\\n', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 32718, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
    "{'base_model': 'mistral-medium', 'prompt_type': 'mistralai', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '\\n', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 32718, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt 3: Language Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## havent thought about it yet lol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:  \n",
    "  \n",
    "Output email .txt  \n",
    "input user name for email sign-off?  \n",
    "Encapsulate all functions  \n",
    "Language Translation prompt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"trial_meeting_minutes_output.txt\",\"w\") as f:\n",
    "#     f.write(llm_reply.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emailing stuff that I havent looked at yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert output file into pdf file to attach to email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "\n",
    "def create_pdf(text, filename='output.pdf'):\n",
    "    # Create a canvas\n",
    "    c = canvas.Canvas(filename, pagesize=letter)\n",
    "    \n",
    "    # Set up font\n",
    "    c.setFont(\"Helvetica\", 12)\n",
    "    \n",
    "    # Split text into lines\n",
    "    lines = text.split('\\n')\n",
    "    \n",
    "    # Set starting y-coordinate\n",
    "    y = 750\n",
    "    \n",
    "    # Write text to the canvas\n",
    "    for line in lines:\n",
    "        c.drawString(50, y, line)\n",
    "        y -= 15  # Adjust for next line\n",
    "        \n",
    "    # Save the canvas\n",
    "    c.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PDF\n",
    "create_pdf(llm_reply.content, \"trial_meeting_minutes_output.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automate Email sending\n",
    "\n",
    "https://keentolearn.medium.com/automating-emails-with-python-a-comprehensive-guide-ba00fa98b92#:~:text=Once%20you%20have%20set%20up,account%2C%20and%20send%20the%20email."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Created junk email for everyone to use\n",
    "email : gpt10.4213.1@outlook.com <br>\n",
    "pw: gpt10email<br>\n",
    "\n",
    "name : gpt<br>\n",
    "surname : 10<br>\n",
    "country : senegal<br>\n",
    "dob : 7 Mar 1989"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Send email without attachment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #This works!\n",
    "# import smtplib\n",
    "\n",
    "# smtp_server = 'smtp-mail.outlook.com'\n",
    "# smtp_port = 587\n",
    "# smtp_username = 'gpt10.4213.1@outlook.com'\n",
    "# smtp_password = 'gpt10email'\n",
    "\n",
    "# from_email = 'gpt10.4213.1@outlook.com'\n",
    "# to_email = 'gpt10.4213.1@outlook.com'\n",
    "# subject = 'Hello, world!'\n",
    "# body = 'This is a test email. using smtplib~~'\n",
    "\n",
    "# message = f'Subject: {subject}\\n\\n{body}'\n",
    "\n",
    "# with smtplib.SMTP(smtp_server, smtp_port) as smtp:\n",
    "#     smtp.starttls()\n",
    "#     smtp.login(smtp_username, smtp_password)\n",
    "#     smtp.sendmail(from_email, to_email, message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smtp_server_mapping = {\n",
    "    \"gmail\":\"smtp.gmail.com\",\n",
    "    \"outlook\":\"smtp-mail.outlook.com\",\n",
    "    \"hotmail\":\"smtp.office365.com\",\n",
    "    \"yahoo\":\"smtp.mail.yahoo.com\",\n",
    "}  #the rest dont care first since different smtp_port # ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Send email with attachment\n",
    "\n",
    "works with a list of email addresses ~ish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.application import MIMEApplication\n",
    "\n",
    "smtp_server = 'smtp-mail.outlook.com'\n",
    "smtp_port = 587\n",
    "smtp_username = 'gpt10.4213.1@outlook.com'\n",
    "smtp_password = 'gpt10email'\n",
    "\n",
    "from_email = 'gpt10.4213.1@outlook.com'\n",
    "to_email = 'gpt10.4213.1@outlook.com'\n",
    "to_email_list = ['gpt10.4213.1@outlook.com',]\n",
    "\n",
    "meeting_date = \"5/3/2024\"\n",
    "subject = f\"{meeting_date} Meeting\"  # meeting_date either user input or LLM search through document.\n",
    "body = f'Please find attached the report. \\n This is a summary of the meeting. \\n {llm_reply2.content}'\n",
    "\n",
    "msg = MIMEMultipart()\n",
    "msg['From'] = from_email\n",
    "# msg['To'] = to_email # send to single user\n",
    "msg['To'] = \", \".join(to_email_list) # send to multiple users\n",
    "msg['Subject'] = subject\n",
    "msg.attach(MIMEText(body))\n",
    "\n",
    "with open('trial_meeting_minutes_output.pdf', 'rb') as f:\n",
    "    attachment = MIMEApplication(f.read(), _subtype='pdf')\n",
    "    attachment.add_header('Content-Disposition', 'attachment', filename='report.pdf')\n",
    "    msg.attach(attachment)\n",
    "\n",
    "with open('trial_meeting_minutes_output.txt', 'rb') as f:\n",
    "    attachment = MIMEApplication(f.read(), _subtype='txt')\n",
    "    attachment.add_header('Content-Disposition', 'attachment', filename='report_friendly.txt')\n",
    "    msg.attach(attachment)\n",
    "\n",
    "with smtplib.SMTP(smtp_server, smtp_port) as smtp:\n",
    "    smtp.starttls()\n",
    "    smtp.login(smtp_username, smtp_password)\n",
    "    smtp.send_message(msg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsa4213env",
   "language": "python",
   "name": "dsa4213env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
