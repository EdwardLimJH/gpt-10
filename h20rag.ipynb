{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load document and start session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "H2O_API_KEY = os.getenv(\"H2O_API_KEY\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2ogpte import H2OGPTE\n",
    "\n",
    "client = H2OGPTE(\n",
    "    address='https://h2ogpte.genai.h2o.ai',\n",
    "    api_key= H2O_API_KEY,\n",
    ")\n",
    "\n",
    "# Create a new collection\n",
    "collection_id = client.create_collection(\n",
    "    name='First meeting transcirpt',\n",
    "    description='Information related to the first group meeting',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Job(id='0761fd06-b19c-42ff-b778-9fbebdf79892', name='', passed=1.0, failed=0.0, progress=1.0, completed=True, canceled=False, date=datetime.datetime(2024, 3, 22, 8, 19, 1, tzinfo=TzInfo(UTC)), kind=<JobKind.IngestUploadsJob: 'IngestUploadsJob'>, statuses=[JobStatus(id='625aa094262e4c0e9c99b12a3e75ada6', status='Collecting done.'), JobStatus(id='16437e3aaf5f42d598304be264ce43f2', status='Collecting done.'), JobStatus(id='e846ed9e50e544479bc13d750ee29b36', status='Indexing done.')], errors=[], last_update_date=datetime.datetime(2024, 3, 22, 8, 19, 13, tzinfo=TzInfo(UTC)), duration='12s')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('meeting_mintues.txt', 'rb') as f:\n",
    "    meeting_data1 = client.upload('meeting_mintues.txt', f)\n",
    "client.ingest_uploads(collection_id, [meeting_data1,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_session_id = client.create_chat_session(collection_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base_model': 'mistralai/Mixtral-8x7B-Instruct-v0.1', 'prompt_type': 'mixtral', 'prompt_dict': {'promptA': '<<SYS>>\\nYou are an AI that follows instructions extremely well and as helpful as possible.\\n<</SYS>>\\n\\n', 'promptB': '<<SYS>>\\nYou are an AI that follows instructions extremely well and as helpful as possible.\\n<</SYS>>\\n\\n', 'PreInstruct': '<s> [INST] ', 'PreInput': None, 'PreResponse': '[/INST]', 'terminate_response': ['[INST]', '</s>'], 'chat_sep': ' ', 'chat_turn_sep': '</s> ', 'humanstr': '[INST]', 'botstr': '[/INST]', 'generates_leading_space': False, 'system_prompt': 'You are an AI that follows instructions extremely well and as helpful as possible.', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 32718, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'h2oai/h2ogpt-4096-llama2-70b-chat', 'prompt_type': 'llama2', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': \"<s>[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\n\", 'PreInput': None, 'PreResponse': '[/INST]', 'terminate_response': ['[INST]', '</s>'], 'chat_sep': ' ', 'chat_turn_sep': ' </s>', 'humanstr': '[INST]', 'botstr': '[/INST]', 'generates_leading_space': False, 'system_prompt': \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\", 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 4046, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'h2oai/h2ogpt-4096-llama2-13b-chat', 'prompt_type': 'llama2', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': \"<s>[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\n\", 'PreInput': None, 'PreResponse': '[/INST]', 'terminate_response': ['[INST]', '</s>'], 'chat_sep': ' ', 'chat_turn_sep': ' </s>', 'humanstr': '[INST]', 'botstr': '[/INST]', 'generates_leading_space': False, 'system_prompt': \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\", 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 4046, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'h2oai/h2ogpt-32k-codellama-34b-instruct', 'prompt_type': 'llama2', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': \"<s>[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\n\", 'PreInput': None, 'PreResponse': '[/INST]', 'terminate_response': ['[INST]', '</s>'], 'chat_sep': ' ', 'chat_turn_sep': ' </s>', 'humanstr': '[INST]', 'botstr': '[/INST]', 'generates_leading_space': False, 'system_prompt': \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\", 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 32718, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'HuggingFaceH4/zephyr-7b-beta', 'prompt_type': 'zephyr', 'prompt_dict': {'promptA': '<|system|>\\nYou are an AI that follows instructions extremely well and as helpful as possible.</s>\\n', 'promptB': '<|system|>\\nYou are an AI that follows instructions extremely well and as helpful as possible.</s>\\n', 'PreInstruct': '<|user|>\\n', 'PreInput': None, 'PreResponse': '</s>\\n<|assistant|>\\n', 'terminate_response': ['<|assistant|>', '</s>'], 'chat_sep': '', 'chat_turn_sep': '</s>\\n', 'humanstr': '<|user|>', 'botstr': '<|assistant|>', 'generates_leading_space': False, 'system_prompt': 'You are an AI that follows instructions extremely well and as helpful as possible.', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 32718, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'NousResearch/Nous-Capybara-34B', 'prompt_type': 'vicuna11', 'prompt_dict': {'promptA': \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. \", 'promptB': \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. \", 'PreInstruct': 'USER: ', 'PreInput': None, 'PreResponse': 'ASSISTANT:', 'terminate_response': ['ASSISTANT:', '</s>'], 'chat_sep': ' ', 'chat_turn_sep': '</s>', 'humanstr': 'USER: ', 'botstr': 'ASSISTANT:', 'generates_leading_space': False, 'system_prompt': \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\", 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 199950, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'claude-2.1', 'prompt_type': 'anthropic', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '\\n', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 199950, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'gpt-3.5-turbo-0613', 'prompt_type': 'openai_chat', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '\\n', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 4046, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'gpt-3.5-turbo-16k-0613', 'prompt_type': 'openai_chat', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '\\n', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 16335, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'gpt-35-turbo-1106', 'prompt_type': 'openai_chat', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '\\n', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 16335, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'gpt-4-1106-preview', 'prompt_type': 'openai_chat', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '\\n', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 127950, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'gemini-pro', 'prompt_type': 'google', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '\\n', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': 'I am a helpful assistant.  I will accurately answer all your questions.', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 32718, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'mistral-small-latest', 'prompt_type': 'mistralai', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '\\n', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 32718, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'mistral-large-latest', 'prompt_type': 'mistralai', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '\\n', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 32718, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'mistral-medium', 'prompt_type': 'mistralai', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '\\n', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 32718, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n"
     ]
    }
   ],
   "source": [
    "llm_list = client.get_llms() # default use first elem, index[0]\n",
    "for llm in llm_list:\n",
    "    print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chat(client, chat_session_id, main_prompt, system_prompt):\n",
    "    with client.connect(chat_session_id) as session:\n",
    "        reply = session.query(\n",
    "            message=main_prompt,\n",
    "            system_prompt=system_prompt,\n",
    "            rag_config={\n",
    "            \"rag_type\": \"rag\", # https://h2oai.github.io/h2ogpte/getting_started.html#advanced-controls-for-document-q-a\n",
    "            },\n",
    "            llm=\"h2oai/h2ogpt-4096-llama2-70b-chat\",\n",
    "        )\n",
    "        return reply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizer and system prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a Meeting Assistant for a software company. You will be given the meeting log in the form of audio, powerpoint \n",
    "or PDF. Your general duties include but are not limited to summarising important points during the meeting, taking note \n",
    "of everyone's deliverables and organising all information in a clear and concise form. Upon reading your output all\n",
    "members should have a clear understanding of the meeting objectives, agenda, and outcomes.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer_prompt = \"\"\"\n",
    "Meeting Assistant Prompt Template:\n",
    "\n",
    "Please retrieve the following details based on the meeting input:\n",
    "\n",
    "1. Attendees: Who attended this meeting? Please list the names and email addresses of participants.\n",
    "\n",
    "2. Meeting Details:\n",
    "    Date and Time: When did the meeting take place?\n",
    "    Duration: How long did the meeting last?\n",
    "\n",
    "3. Meeting Purpose: \n",
    "    Objective: What is the primary objective or purpose of the meeting?\n",
    "    Desired Outcome: What outcomes do you hope to achieve by the end of the meeting?\n",
    "\n",
    "4. Meeting summary: Provide a concise summary of the entire meeting by listing out all the main highlights\n",
    "                    such that participants not in the meeting will be able to understand what happened during the meeting.\n",
    "    \n",
    "5. Action Items:\n",
    "    Deliverables: What is the distribution of workload discussed during the meeting?\n",
    "    Assign: For each attendee list out in detail all the work assigned to them, be as precise and detailed as possible.\n",
    "\n",
    "Present points 1-5 in the following format: \n",
    "{\"Attendees\": \"\", \"Meeting Details\": [\"Date and Time\":\"\", \"Duration\":\"\"], \n",
    "\"Meeting Details\": [\"Objective\":\"\", \"Desired Outcome\":\"\"], \"Meeting summary\":\"\", \n",
    "\"Action Items\": [\"Deliverables\":\"\", \"Assign\":\"\"]\n",
    "} \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attendees: Ryan_Edward, Jeremy_PH, Hannah Nga, Chan Yi Ru Micole, Ben_CH, Eugene_YJ\n",
      "\n",
      "Meeting Details:\n",
      "Date and Time: 22:17:11 - 22:55:14\n",
      "Duration: 38 minutes\n",
      "\n",
      "Meeting Purpose:\n",
      "Objective: To discuss the development of a chatbot that can summarize meetings\n",
      "Desired Outcome: To have a clear understanding of the meeting objectives, agenda, and outcomes\n",
      "\n",
      "Meeting Summary:\n",
      "\n",
      "* Discussed the possibility of creating a chatbot that can summarize meetings\n",
      "* Considered the idea of using a bare bones model and pushing it out by the next meeting\n",
      "* Talked about the importance of having a clear understanding of the meeting objectives, agenda, and outcomes\n",
      "* Mentioned the possibility of extending the chatbot's functionality to include sentiment analysis\n",
      "* Decided to have a follow-up meeting to discuss the progress of the chatbot development\n",
      "\n",
      "Action Items:\n",
      "\n",
      "Deliverables:\n",
      "\n",
      "* Develop a chatbot that can summarize meetings\n",
      "* Create a user interface for the chatbot\n",
      "* Test the chatbot and make any necessary adjustments\n",
      "\n",
      "Assign:\n",
      "\n",
      "* Ryan_Edward: Lead the development of the chatbot\n",
      "* Jeremy_PH: Assist with the development of the chatbot\n",
      "* Hannah Nga: Create the user interface for the chatbot\n",
      "* Chan Yi Ru Micole: Test the chatbot and provide feedback\n",
      "* Ben_CH: Provide support for the development of the chatbot\n",
      "* Eugene_YJ: Assist with the testing of the chatbot\n"
     ]
    }
   ],
   "source": [
    "meeting_summary_response = rag_chat(client, chat_session_id, summarizer_prompt, system_prompt)\n",
    "print(meeting_summary_response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
