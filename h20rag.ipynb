{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load document and start session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "H2O_API_KEY = os.getenv(\"H2O_API_KEY\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Server version 1.4.7 doesn't match client version 1.4.0: unexpected errors may occur.\n",
      "Please install the correct version of H2OGPTE with `pip install h2ogpte==1.4.7`.\n",
      "You can enable strict version checking by passing strict_version_check=True.\n"
     ]
    }
   ],
   "source": [
    "from h2ogpte import H2OGPTE\n",
    "\n",
    "client = H2OGPTE(\n",
    "    address='https://h2ogpte.genai.h2o.ai',\n",
    "    api_key= H2O_API_KEY,\n",
    ")\n",
    "\n",
    "# Create a new collection\n",
    "collection_id = client.create_collection(\n",
    "    name='First meeting transcirpt',\n",
    "    description='Information related to the first group meeting',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Job(id='b9cd4160-55dc-486b-8d3e-6051af96a65b', name='Adding documents', passed=1.0, failed=0.0, progress=1.0, completed=True, canceled=False, date=datetime.datetime(2024, 3, 22, 14, 1, 32, tzinfo=TzInfo(UTC)), kind=<JobKind.IngestUploadsJob: 'IngestUploadsJob'>, statuses=[JobStatus(id='16b14b9d57f94c39b4446ff557971f4c', status='Indexing done.'), JobStatus(id='7294d974664446aea89605922b1dca67', status='Collecting done.')], errors=[], last_update_date=datetime.datetime(2024, 3, 22, 14, 1, 59, tzinfo=TzInfo(UTC)), duration='27s')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('meeting_mintues.txt', 'rb') as f:\n",
    "    meeting_data1 = client.upload('meeting_mintues.txt', f)\n",
    "client.ingest_uploads(collection_id, [meeting_data1,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_session_id = client.create_chat_session(collection_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base_model': 'mistralai/Mixtral-8x7B-Instruct-v0.1', 'prompt_type': 'mixtral', 'prompt_dict': {'promptA': '<<SYS>>\\nYou are an AI that follows instructions extremely well and as helpful as possible.\\n<</SYS>>\\n\\n', 'promptB': '<<SYS>>\\nYou are an AI that follows instructions extremely well and as helpful as possible.\\n<</SYS>>\\n\\n', 'PreInstruct': '<s> [INST] ', 'PreInput': None, 'PreResponse': '[/INST]', 'terminate_response': ['[INST]', '</s>'], 'chat_sep': ' ', 'chat_turn_sep': '</s> ', 'humanstr': '[INST]', 'botstr': '[/INST]', 'generates_leading_space': False, 'system_prompt': 'You are an AI that follows instructions extremely well and as helpful as possible.', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 32718, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'h2oai/h2ogpt-4096-llama2-70b-chat', 'prompt_type': 'llama2', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': \"<s>[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\n\", 'PreInput': None, 'PreResponse': '[/INST]', 'terminate_response': ['[INST]', '</s>'], 'chat_sep': ' ', 'chat_turn_sep': ' </s>', 'humanstr': '[INST]', 'botstr': '[/INST]', 'generates_leading_space': False, 'system_prompt': \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\", 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 4046, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'h2oai/h2ogpt-4096-llama2-13b-chat', 'prompt_type': 'llama2', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': \"<s>[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\n\", 'PreInput': None, 'PreResponse': '[/INST]', 'terminate_response': ['[INST]', '</s>'], 'chat_sep': ' ', 'chat_turn_sep': ' </s>', 'humanstr': '[INST]', 'botstr': '[/INST]', 'generates_leading_space': False, 'system_prompt': \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\", 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 4046, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'h2oai/h2ogpt-32k-codellama-34b-instruct', 'prompt_type': 'llama2', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': \"<s>[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\n\", 'PreInput': None, 'PreResponse': '[/INST]', 'terminate_response': ['[INST]', '</s>'], 'chat_sep': ' ', 'chat_turn_sep': ' </s>', 'humanstr': '[INST]', 'botstr': '[/INST]', 'generates_leading_space': False, 'system_prompt': \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\", 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 32718, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'HuggingFaceH4/zephyr-7b-beta', 'prompt_type': 'zephyr', 'prompt_dict': {'promptA': '<|system|>\\nYou are an AI that follows instructions extremely well and as helpful as possible.</s>\\n', 'promptB': '<|system|>\\nYou are an AI that follows instructions extremely well and as helpful as possible.</s>\\n', 'PreInstruct': '<|user|>\\n', 'PreInput': None, 'PreResponse': '</s>\\n<|assistant|>\\n', 'terminate_response': ['<|assistant|>', '</s>'], 'chat_sep': '', 'chat_turn_sep': '</s>\\n', 'humanstr': '<|user|>', 'botstr': '<|assistant|>', 'generates_leading_space': False, 'system_prompt': 'You are an AI that follows instructions extremely well and as helpful as possible.', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 32718, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'NousResearch/Nous-Capybara-34B', 'prompt_type': 'vicuna11', 'prompt_dict': {'promptA': \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. \", 'promptB': \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. \", 'PreInstruct': 'USER: ', 'PreInput': None, 'PreResponse': 'ASSISTANT:', 'terminate_response': ['ASSISTANT:', '</s>'], 'chat_sep': ' ', 'chat_turn_sep': '</s>', 'humanstr': 'USER: ', 'botstr': 'ASSISTANT:', 'generates_leading_space': False, 'system_prompt': \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\", 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 199950, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'claude-2.1', 'prompt_type': 'anthropic', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '\\n', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 199950, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'gpt-3.5-turbo-0613', 'prompt_type': 'openai_chat', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '\\n', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 4046, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'gpt-3.5-turbo-16k-0613', 'prompt_type': 'openai_chat', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '\\n', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 16335, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'gpt-35-turbo-1106', 'prompt_type': 'openai_chat', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '\\n', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 16335, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'gpt-4-1106-preview', 'prompt_type': 'openai_chat', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '\\n', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 127950, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'gemini-pro', 'prompt_type': 'google', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '\\n', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': 'I am a helpful assistant.  I will accurately answer all your questions.', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 32718, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'mistral-small-latest', 'prompt_type': 'mistralai', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '\\n', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 32718, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'mistral-large-latest', 'prompt_type': 'mistralai', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '\\n', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 32718, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'mistral-medium', 'prompt_type': 'mistralai', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '\\n', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 32718, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n"
     ]
    }
   ],
   "source": [
    "llm_list = client.get_llms() # default use first elem, index[0]\n",
    "for llm in llm_list:\n",
    "    print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chat(client, chat_session_id, main_prompt, system_prompt):\n",
    "    with client.connect(chat_session_id) as session:\n",
    "        reply = session.query(\n",
    "            message=main_prompt,\n",
    "            system_prompt=system_prompt,\n",
    "            rag_config={\n",
    "            \"rag_type\": \"rag\", # https://h2oai.github.io/h2ogpte/getting_started.html#advanced-controls-for-document-q-a\n",
    "            },\n",
    "            llm=\"h2oai/h2ogpt-4096-llama2-70b-chat\",\n",
    "        )\n",
    "        return reply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizer and system prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a Meeting Assistant for a software company. You will be given the meeting log in the form of audio, powerpoint \n",
    "or PDF. Your general duties include but are not limited to summarising important points during the meeting, taking note \n",
    "of everyone's deliverables and organising all information in a clear and concise form. Upon reading your output all\n",
    "members should have a clear understanding of the meeting objectives, agenda, and outcomes.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer_prompt = \"\"\"\n",
    "Meeting Assistant Prompt Template:\n",
    "\n",
    "Please retrieve the following details based on the meeting input:\n",
    "\n",
    "1. Attendees: Who attended this meeting? Please list the names and email addresses of participants.\n",
    "\n",
    "2. Meeting Details:\n",
    "    Date and Time: When did the meeting take place?\n",
    "    Duration: How long did the meeting last?\n",
    "\n",
    "3. Meeting Purpose: \n",
    "    Objective: What is the primary objective or purpose of the meeting?\n",
    "    Desired Outcome: What outcomes do you hope to achieve by the end of the meeting?\n",
    "\n",
    "4. Meeting summary: Provide a concise summary of the entire meeting by listing out all the main highlights\n",
    "                    such that participants not in the meeting will be able to understand what happened during the meeting.\n",
    "    \n",
    "5. Action Items:\n",
    "    Deliverables: What is the distribution of workload discussed during the meeting?\n",
    "    Assign: For each attendee, list out in detail all the work assigned to them, be as precise and detailed as possible.\n",
    "\n",
    "Present points 1-5 in the following format: \n",
    "{\"Attendees\": \"\", \"Meeting Details\": [\"Date and Time\":\"\", \"Duration\":\"\"], \n",
    "\"Meeting Details\": [\"Objective\":\"\", \"Desired Outcome\":\"\"], \"Meeting summary\":\"\", \n",
    "\"Action Items\": [\"Deliverables\":\"\", \"Assign\":\"\"]\n",
    "} \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attendees: Ryan_Edward, Eugene_YJ, Hannah Nga, Ben_CH, Jeremy_PH, Chan Yi Ru Micole\n",
      "\n",
      "Meeting Details:\n",
      "Date and Time: 22:18:02\n",
      "Duration: Not specified\n",
      "\n",
      "Meeting Purpose:\n",
      "Objective: To discuss the development of a chatbot for a software company\n",
      "Desired Outcome: To outline the key features and functionalities of the chatbot, and to assign tasks to team members for its development\n",
      "\n",
      "Meeting Summary:\n",
      "The meeting discussed the development of a chatbot for a software company. The team discussed the key features and functionalities of the chatbot, including the ability to summarize meetings, generate meeting minutes, and provide translations. The team also discussed the distribution of workload and assigned tasks to team members for the development of the chatbot.\n",
      "\n",
      "Action Items:\n",
      "Deliverables:\n",
      "\n",
      "* Develop a chatbot for a software company\n",
      "* Include the following features:\n",
      "\t+ Summarize meetings\n",
      "\t+ Generate meeting minutes\n",
      "\t+ Provide translations\n",
      "\n",
      "Assign:\n",
      "\n",
      "* Ryan_Edward: Lead the development of the chatbot\n",
      "* Eugene_YJ: Work on the summarization feature\n",
      "* Hannah Nga: Work on the generation of meeting minutes\n",
      "* Ben_CH: Work on the translation feature\n",
      "* Jeremy_PH: Assist with the development of the chatbot\n",
      "* Chan Yi Ru Micole: Assist with the development of the chatbot and provide input on the meeting summary feature.\n"
     ]
    }
   ],
   "source": [
    "meeting_summary_response = rag_chat(client, chat_session_id, summarizer_prompt, system_prompt)\n",
    "print(meeting_summary_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_prompt = \"\"\"\n",
    "Sentiment Prompt Template:\n",
    "\n",
    "Please retrieve the following details based on the meeting input:\n",
    "\n",
    "1. Attendees: Who attended this meeting? Please list the names and email addresses of participants.\n",
    "    \n",
    "2. Action Items:\n",
    "    Deliverables: What is the distribution of workload discussed during the meeting?\n",
    "    Assign: For each attendee, list out in detail all the work assigned to them, be as precise and detailed as possible.\n",
    "            For each work assigned arange them in order of importance\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, I can help you with that. Here's the information you requested:\n",
      "\n",
      "1. Attendees:\n",
      "\n",
      "The following individuals attended the meeting:\n",
      "\n",
      "* Ryan_Edward\n",
      "* Chan Yi Ru Micole\n",
      "* Hannah Nga\n",
      "* Eugene_YJ\n",
      "* Jeremy_PH\n",
      "* Ben_CH\n",
      "\n",
      "Their email addresses are not specified in the meeting logs provided.\n",
      "\n",
      "2. Action Items:\n",
      "\n",
      "a. Deliverables:\n",
      "\n",
      "The meeting discussed the following deliverables:\n",
      "\n",
      "* Meeting minute summary\n",
      "* Actionable items\n",
      "* Sentiment analysis\n",
      "* Translation\n",
      "\n",
      "b. Assignments:\n",
      "\n",
      "The meeting assigned the following tasks to specific attendees:\n",
      "\n",
      "* Ryan_Edward:\n",
      "\t+ Create a home page for users to upload files.\n",
      "\t+ Implement sentiment analysis.\n",
      "\t+ Incorporate translation.\n",
      "\t+ Regenerate the whole meeting.\n",
      "* Chan Yi Ru Micole:\n",
      "\t+ Develop a pipeline.\n",
      "\t+ Implement sentiment analysis.\n",
      "\t+ Incorporate translation.\n",
      "* Hannah Nga:\n",
      "\t+ Create a task list.\n",
      "\t+ Assign tasks to users.\n",
      "\t+ Determine priority allocation.\n",
      "* Eugene_YJ:\n",
      "\t+ Implement automated follow-ups.\n",
      "* Jeremy_PH:\n",
      "\t+ Work on the sentiment analysis.\n",
      "* Ben_CH:\n",
      "\t+ Assist with the task list.\n",
      "\n",
      "The importance of each task is not explicitly stated in the meeting logs provided.\n"
     ]
    }
   ],
   "source": [
    "meeting_summary_response = rag_chat(client, chat_session_id, sentiment_prompt, system_prompt)\n",
    "print(meeting_summary_response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
