{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load document and start session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-y51R9UjZuoMJydh1MOEBqnJlxa3DtpOtnwDKEI6slV0na7Cg\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "H2O_API_KEY = os.getenv(\"H2O_API_KEY\") \n",
    "print(H2O_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Server version 1.4.9 doesn't match client version 1.4.0: unexpected errors may occur.\n",
      "Please install the correct version of H2OGPTE with `pip install h2ogpte==1.4.9`.\n",
      "You can enable strict version checking by passing strict_version_check=True.\n",
      "949b0c36-ad0e-4f39-9687-5c90695cfb6b\n"
     ]
    }
   ],
   "source": [
    "from h2ogpte import H2OGPTE\n",
    "\n",
    "client = H2OGPTE(\n",
    "    address='https://h2ogpte.genai.h2o.ai',\n",
    "    api_key= H2O_API_KEY,\n",
    ")\n",
    "\n",
    "# Create a new collection\n",
    "collection_id = client.create_collection(\n",
    "    name='First meeting transcirpt',\n",
    "    description='Information related to the first group meeting',\n",
    ")\n",
    "print(collection_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Job(id='79a0a71a-f63e-4c0b-9a25-7430622e166e', name='Adding documents', passed=1.0, failed=0.0, progress=1.0, completed=True, canceled=False, date=datetime.datetime(2024, 3, 28, 3, 59, 16, tzinfo=TzInfo(UTC)), kind=<JobKind.IngestUploadsJob: 'IngestUploadsJob'>, statuses=[JobStatus(id='f6f52470dca241bbb96b6ff5d2649e4a', status='Indexing done.'), JobStatus(id='36fea923bf79437c86bd632a60267d92', status='Collecting done.')], errors=[], last_update_date=datetime.datetime(2024, 3, 28, 3, 59, 28, tzinfo=TzInfo(UTC)), duration='12s')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('meeting_mintues.txt', 'rb') as f:\n",
    "    meeting_data1 = client.upload('meeting_mintues.txt', f)\n",
    "client.ingest_uploads(collection_id, [meeting_data1,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_session_id = client.create_chat_session(collection_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base_model': 'mistralai/Mixtral-8x7B-Instruct-v0.1', 'prompt_type': 'mixtral', 'prompt_dict': {'promptA': '<<SYS>>\\nYou are an AI that follows instructions extremely well and as helpful as possible.\\n<</SYS>>\\n\\n', 'promptB': '<<SYS>>\\nYou are an AI that follows instructions extremely well and as helpful as possible.\\n<</SYS>>\\n\\n', 'PreInstruct': '<s> [INST] ', 'PreInput': None, 'PreResponse': '[/INST]', 'terminate_response': ['[INST]', '</s>'], 'chat_sep': ' ', 'chat_turn_sep': '</s> ', 'humanstr': '[INST]', 'botstr': '[/INST]', 'generates_leading_space': False, 'system_prompt': 'You are an AI that follows instructions extremely well and as helpful as possible.', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 32718, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'h2oai/h2ogpt-4096-llama2-70b-chat', 'prompt_type': 'llama2', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': \"<s>[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\n\", 'PreInput': None, 'PreResponse': '[/INST]', 'terminate_response': ['[INST]', '</s>'], 'chat_sep': ' ', 'chat_turn_sep': ' </s>', 'humanstr': '[INST]', 'botstr': '[/INST]', 'generates_leading_space': False, 'system_prompt': \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\", 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 4046, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'h2oai/h2ogpt-4096-llama2-13b-chat', 'prompt_type': 'llama2', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': \"<s>[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\n\", 'PreInput': None, 'PreResponse': '[/INST]', 'terminate_response': ['[INST]', '</s>'], 'chat_sep': ' ', 'chat_turn_sep': ' </s>', 'humanstr': '[INST]', 'botstr': '[/INST]', 'generates_leading_space': False, 'system_prompt': \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\", 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 4046, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'h2oai/h2ogpt-32k-codellama-34b-instruct', 'prompt_type': 'llama2', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': \"<s>[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\n\", 'PreInput': None, 'PreResponse': '[/INST]', 'terminate_response': ['[INST]', '</s>'], 'chat_sep': ' ', 'chat_turn_sep': ' </s>', 'humanstr': '[INST]', 'botstr': '[/INST]', 'generates_leading_space': False, 'system_prompt': \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\", 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 32718, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'HuggingFaceH4/zephyr-7b-beta', 'prompt_type': 'zephyr', 'prompt_dict': {'promptA': '<|system|>\\nYou are an AI that follows instructions extremely well and as helpful as possible.</s>\\n', 'promptB': '<|system|>\\nYou are an AI that follows instructions extremely well and as helpful as possible.</s>\\n', 'PreInstruct': '<|user|>\\n', 'PreInput': None, 'PreResponse': '</s>\\n<|assistant|>\\n', 'terminate_response': ['<|assistant|>', '</s>'], 'chat_sep': '', 'chat_turn_sep': '</s>\\n', 'humanstr': '<|user|>', 'botstr': '<|assistant|>', 'generates_leading_space': False, 'system_prompt': 'You are an AI that follows instructions extremely well and as helpful as possible.', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 32718, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'NousResearch/Nous-Capybara-34B', 'prompt_type': 'vicuna11', 'prompt_dict': {'promptA': \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. \", 'promptB': \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. \", 'PreInstruct': 'USER: ', 'PreInput': None, 'PreResponse': 'ASSISTANT:', 'terminate_response': ['ASSISTANT:', '</s>'], 'chat_sep': ' ', 'chat_turn_sep': '</s>', 'humanstr': 'USER: ', 'botstr': 'ASSISTANT:', 'generates_leading_space': False, 'system_prompt': \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\", 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 199950, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'claude-2.1', 'prompt_type': 'anthropic', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 199950, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'gpt-3.5-turbo-0613', 'prompt_type': 'openai_chat', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 4046, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'gpt-3.5-turbo-16k-0613', 'prompt_type': 'openai_chat', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 16335, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'gpt-35-turbo-1106', 'prompt_type': 'openai_chat', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 16335, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'gpt-4-1106-preview', 'prompt_type': 'openai_chat', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 127950, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'gemini-pro', 'prompt_type': 'google', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': 'I am a helpful assistant.  I will accurately answer all your questions.', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 30670, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'mistral-small-latest', 'prompt_type': 'mistralai', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 32518, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'mistral-large-latest', 'prompt_type': 'mistralai', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 32518, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'mistral-medium', 'prompt_type': 'mistralai', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 32518, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n"
     ]
    }
   ],
   "source": [
    "llm_list = client.get_llms() # default use first elem, index[0]\n",
    "for llm in llm_list:\n",
    "    print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chat(client, chat_session_id, main_prompt, system_prompt):\n",
    "    with client.connect(chat_session_id) as session:\n",
    "        reply = session.query(\n",
    "            message=main_prompt,\n",
    "            system_prompt=system_prompt,\n",
    "            rag_config={\n",
    "            \"rag_type\": \"rag\", # https://h2oai.github.io/h2ogpte/getting_started.html#advanced-controls-for-document-q-a\n",
    "            },\n",
    "            llm=\"h2oai/h2ogpt-4096-llama2-70b-chat\",\n",
    "        )\n",
    "        return reply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizer and system prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a Meeting Assistant for a software company. You will be given the meeting log in the form of audio, powerpoint \n",
    "or PDF. Your general duties include but are not limited to summarising important points during the meeting, taking note \n",
    "of everyone's deliverables and organising all information in a clear and concise form. Upon reading your output all\n",
    "members should have a clear understanding of the meeting objectives, agenda, and outcomes.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer_prompt = \"\"\"\n",
    "Meeting Assistant Prompt Template:\n",
    "\n",
    "Please retrieve the following details based on the meeting input:\n",
    "\n",
    "1. Attendees: Who attended this meeting? Please list the names and email addresses of participants.\n",
    "\n",
    "2. Meeting Details:\n",
    "    Date and Time: When did the meeting take place?\n",
    "    Duration: How long did the meeting last?\n",
    "\n",
    "3. Meeting Purpose: \n",
    "    Objective: What is the primary objective or purpose of the meeting?\n",
    "    Desired Outcome: What outcomes do you hope to achieve by the end of the meeting?\n",
    "\n",
    "4. Meeting summary: Provide a concise summary of the entire meeting by listing out all the main highlights\n",
    "                    such that participants not in the meeting will be able to understand what happened during the meeting.\n",
    "    \n",
    "5. Action Items:\n",
    "    Deliverables: What is the distribution of workload discussed during the meeting?\n",
    "    Assign: For each attendee, list out in detail all the work assigned to them, be as precise and detailed as possible.\n",
    "\n",
    "\n",
    "Present points 1-5 in the following JSON format: \n",
    "{\"Attendees\": \"\", \"Meeting Details\": {\"Date and Time\":\"\", \"Duration\":\"\"}, \"Meeting Details\": {\"Objective\":\"\", \"Desired Outcome\":\"\"}, \"Meeting summary\": \"\", \"Action Items\": {\"Deliverables\":\"\", \"Assign\":\"\"}} \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "meeting_summary_response = rag_chat(client, chat_session_id, summarizer_prompt, system_prompt)\n",
    "# print(meeting_summary_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jeremy_PH', 'Hannah Nga', 'Ryan_Edward', 'Chan Yi Ru Micole', 'Ben_CH', 'Eugene_YJ']\n",
      "{'Date and Time': '22:17:11', 'Duration': '22:54:59'}\n",
      "22:17:11\n",
      "22:54:59\n",
      "{'Objective': 'To discuss the development of a chatbot that can summarize meetings', 'Desired Outcome': 'To have a clear understanding of the meeting objectives, agenda, and outcomes'}\n",
      "To discuss the development of a chatbot that can summarize meetings\n",
      "To have a clear understanding of the meeting objectives, agenda, and outcomes\n",
      "The meeting discussed the development of a chatbot that can summarize meetings. The chatbot will be able to summarize the meeting minutes and provide a concise summary of the main highlights. The meeting also discussed the distribution of workload and assigned tasks to each attendee.\n",
      "[{'Name': 'Jeremy_PH', 'Tasks': ['Provide a summary of the meeting minutes']}, {'Name': 'Hannah Nga', 'Tasks': ['Assist in the development of the chatbot']}, {'Name': 'Ryan_Edward', 'Tasks': ['Develop the chatbot']}, {'Name': 'Chan Yi Ru Micole', 'Tasks': ['Test the chatbot']}, {'Name': 'Ben_CH', 'Tasks': ['Provide feedback on the chatbot']}, {'Name': 'Eugene_YJ', 'Tasks': ['Assist in the development of the chatbot']}]\n"
     ]
    }
   ],
   "source": [
    "json_object = json.loads(meeting_summary_response.content)\n",
    "Attendees = json_object['Attendees']\n",
    "print(Attendees)\n",
    "Meeting_Details = json_object['Meeting Details']\n",
    "Date_and_Time = Meeting_Details['Date and Time']\n",
    "Duration = Meeting_Details['Duration']\n",
    "print(Meeting_Details)\n",
    "print(Date_and_Time)\n",
    "print(Duration)\n",
    "Meeting_Purpose = json_object['Meeting Purpose']\n",
    "Objective = Meeting_Purpose['Objective']\n",
    "Desired_Outcome = Meeting_Purpose['Desired Outcome']\n",
    "print(Meeting_Purpose)\n",
    "print(Objective)\n",
    "print(Desired_Outcome)\n",
    "Meeting_summary = json_object['Meeting summary']\n",
    "print(Meeting_summary)\n",
    "Action_Items = json_object['Action Items']['Assign']\n",
    "print(Action_Items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_sentiment_prompt = \"\"\"\n",
    "Sentiment Prompt Template:\n",
    "\n",
    "Please retrieve the following details based on the meeting input:\n",
    "\n",
    "1. Attendees: Who attended this meeting? Please list the names and email addresses of participants.\n",
    "    \n",
    "2. Action Items:\n",
    "    Deliverables: What is the distribution of workload discussed during the meeting?\n",
    "    Assign: For each attendee, list out in detail all the work assigned to them, be as precise and detailed as possible.\n",
    "            For each work assigned arange them in order of importance\n",
    "\n",
    "Present points 1 and 2 in the following JSON format: \n",
    "{\"Attendees\": \"\", \"Action Items\": [\"Deliverables\":\"\", \"Assign\":\"\"]} \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_prompt = \"\"\"\n",
    "Meeting Assistant Prompt Template:\n",
    "\n",
    "Please retrieve the following details based on the meeting input:\n",
    "\n",
    "1. Attendees: Who attended this meeting? Please list the names and email addresses of participants.\n",
    "\n",
    "2. Meeting Details:\n",
    "    Date and Time: When did the meeting take place?\n",
    "    Duration: How long did the meeting last?\n",
    "\n",
    "3. Meeting Purpose: \n",
    "    Objective: What is the primary objective or purpose of the meeting?\n",
    "    Desired Outcome: What outcomes do you hope to achieve by the end of the meeting?\n",
    "\n",
    "4. Meeting summary: Provide a concise summary of the entire meeting by listing out all the main highlights\n",
    "                    such that participants not in the meeting will be able to understand what happened during the meeting.\n",
    "    \n",
    "5. Action Items:\n",
    "    Deliverables: What is the distribution of workload discussed during the meeting?\n",
    "    Assign: For each attendee, list out in detail all the work assigned to them, be as precise and detailed as possible. Rank the assignment by either 'Low', 'Medium', or 'High' prioriry.\n",
    "\n",
    "\n",
    "Present points 1-5 in mandarin chinese language and the following JSON format: \n",
    "{\"Attendees\": \"\", \"Meeting Details\": {\"Date and Time\":\"\", \"Duration\":\"\"}, \"Meeting Details\": {\"Objective\":\"\", \"Desired Outcome\":\"\"}, \"Meeting summary\": \"\", \"Action Items\": {\"Deliverables\":\"\", \"Assign\":\"\"}} \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"Attendees\": [\n",
      "\"Ben_CH\",\n",
      "\"Eugene_YJ\",\n",
      "\"Hannah Nga\",\n",
      "\"Jeremy_PH\",\n",
      "\"Ryan_Edward\",\n",
      "\"Chan Yi Ru Micole\"\n",
      "],\n",
      "\"Meeting Details\": {\n",
      "\"Date and Time\": \"22:18:02\",\n",
      "\"Duration\": \"22:54:59\"\n",
      "},\n",
      "\"Meeting Purpose\": {\n",
      "\"Objective\": \"To discuss the development of a chatbot for a software company\",\n",
      "\"Desired Outcome\": \"To outline the key features and functionalities of the chatbot, and to assign tasks to team members for its development\"\n",
      "},\n",
      "\"Meeting Summary\": [\n",
      "\"Discussion of the chatbot's purpose and objectives\",\n",
      "\"Outline of the chatbot's key features and functionalities\",\n",
      "\"Assignment of tasks to team members for the chatbot's development\"\n",
      "],\n",
      "\"Action Items\": {\n",
      "\"Deliverables\": \"Development of a chatbot for a software company\",\n",
      "\"Assign\": [\n",
      "{\n",
      "\"Name\": \"Ryan_Edward\",\n",
      "\"Task\": \"Develop a summary of the meeting and distribute it to all team members\",\n",
      "\"Priority\": \"High\"\n",
      "},\n",
      "{\n",
      "\"Name\": \"Chan Yi Ru Micole\",\n",
      "\"Task\": \"Develop a bare bones model of the chatbot and push it out by the next meeting\",\n",
      "\"Priority\": \"High\"\n",
      "},\n",
      "{\n",
      "\"Name\": \"Eugene_YJ\",\n",
      "\"Task\": \"Assist in the development of the chatbot's language translation feature\",\n",
      "\"Priority\": \"Medium\"\n",
      "},\n",
      "{\n",
      "\"Name\": \"Hannah Nga\",\n",
      "\"Task\": \"Assist in the development of the chatbot's sentiment analysis feature\",\n",
      "\"Priority\": \"Medium\"\n",
      "},\n",
      "{\n",
      "\"Name\": \"Jeremy_PH\",\n",
      "\"Task\": \"Assist in the development of the chatbot's task assignment feature\",\n",
      "\"Priority\": \"Medium\"\n",
      "}\n",
      "]\n",
      "}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "meeting_summary_response = rag_chat(client, chat_session_id, sentiment_prompt, system_prompt)\n",
    "print(meeting_summary_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language translation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate = \"\"\"\n",
    "{\n",
    "\"Attendees\": [\n",
    "\"Ben_CH\",\n",
    "\"Eugene_YJ\",\n",
    "\"Hannah Nga\",\n",
    "\"Jeremy_PH\",\n",
    "\"Ryan_Edward\",\n",
    "\"Chan Yi Ru Micole\"\n",
    "],\n",
    "\"Meeting Details\": {\n",
    "\"Date and Time\": \"22:18:02\",\n",
    "\"Duration\": \"22:54:59\"\n",
    "},\n",
    "\"Meeting Purpose\": {\n",
    "\"Objective\": \"To discuss the development of a chatbot for a software company\",\n",
    "\"Desired Outcome\": \"To outline the key features and functionalities of the chatbot, and to assign tasks to team members for its development\"\n",
    "},\n",
    "\"Meeting Summary\": [\n",
    "\"Discussion of the chatbot's purpose and objectives\",\n",
    "\"Outline of the chatbot's key features and functionalities\",\n",
    "\"Assignment of tasks to team members for the chatbot's development\"\n",
    "],\n",
    "\"Action Items\": {\n",
    "\"Deliverables\": \"Development of a chatbot for a software company\",\n",
    "\"Assign\": [\n",
    "{\n",
    "\"Name\": \"Ryan_Edward\",\n",
    "\"Task\": \"Develop a summary of the meeting and distribute it to all team members\",\n",
    "\"Priority\": \"High\"\n",
    "},\n",
    "{\n",
    "\"Name\": \"Chan Yi Ru Micole\",\n",
    "\"Task\": \"Develop a bare bones model of the chatbot and push it out by the next meeting\",\n",
    "\"Priority\": \"High\"\n",
    "},\n",
    "{\n",
    "\"Name\": \"Eugene_YJ\",\n",
    "\"Task\": \"Assist in the development of the chatbot's language translation feature\",\n",
    "\"Priority\": \"Medium\"\n",
    "},\n",
    "{\n",
    "\"Name\": \"Hannah Nga\",\n",
    "\"Task\": \"Assist in the development of the chatbot's sentiment analysis feature\",\n",
    "\"Priority\": \"Medium\"\n",
    "},\n",
    "{\n",
    "\"Name\": \"Jeremy_PH\",\n",
    "\"Task\": \"Assist in the development of the chatbot's task assignment feature\",\n",
    "\"Priority\": \"Medium\"\n",
    "}\n",
    "]\n",
    "}\n",
    "}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_prompt = \"\"\"\n",
    "Translate the input question to mandarin chinese langauge and output in the following JSON format:\n",
    "\n",
    "{\"参与者\": \"\", \"会议详细信息\": {\"日期和时间\":\"\", \"持续时间\":\"\"}, \"会议目的\": {\"目标\":\"\", \"期望结果\":\"\"}, \"会议总结\": \"\", \"行动项\": {\"交付物\":\"\", \"分配\":\"\"}} \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = \"h2oai/h2ogpt-4096-llama2-70b-chat\"\n",
    "answer = client.answer_question(question=translate, system_prompt= translation_prompt, llm=llm).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"参与者\": [\n",
      "\"Ben_CH\",\n",
      "\"Eugene_YJ\",\n",
      "\"Hannah Nga\",\n",
      "\"Jeremy_PH\",\n",
      "\"Ryan_Edward\",\n",
      "\"Chan Yi Ru Micole\"\n",
      "],\n",
      "\"会议详细信息\": {\n",
      "\"日期和时间\": \"22:18:02\",\n",
      "\"持续时间\": \"22:54:59\"\n",
      "},\n",
      "\"会议目的\": {\n",
      "\"目标\": \"讨论软件公司的 chatbot 开发\",\n",
      "\"期望结果\": \"outline  chatbot 的主要特性和功能，并分配任务给团队成员进行其开发\"\n",
      "},\n",
      "\"会议总结\": [\n",
      "\"讨论 chatbot 的目的和目标\",\n",
      "\"Outline  chatbot 的主要特性和功能\",\n",
      "\"分配任务给团队成员进行 chatbot 的开发\"\n",
      "],\n",
      "\"行动项\": {\n",
      "\"交付物\": \"软件公司的 chatbot 开发\",\n",
      "\"分配\": [\n",
      "{\n",
      "\"名称\": \"Ryan_Edward\",\n",
      "\"任务\": \"编写会议纪要并分发给所有团队成员\",\n",
      "\"优先级\": \"高\"\n",
      "},\n",
      "{\n",
      "\"名称\": \"Chan Yi Ru Micole\",\n",
      "\"任务\": \"开发 chatbot 的基础模型并在下一次会议前推出\",\n",
      "\"优先级\": \"高\"\n",
      "},\n",
      "{\n",
      "\"名称\": \"Eugene_YJ\",\n",
      "\"任务\": \"协助 chatbot 的语言翻译功能开发\",\n",
      "\"优先级\": \"中等\"\n",
      "},\n",
      "{\n",
      "\"名称\": \"Hannah Nga\",\n",
      "\"任务\": \"协助 chatbot 的情感分析功能开发\",\n",
      "\"优先级\": \"中等\"\n",
      "},\n",
      "{\n",
      "\"名称\": \"Jeremy_PH\",\n",
      "\"任务\": \"协助 chatbot 的任务分配功能开发\",\n",
      "\"优先级\": \"中等\"\n",
      "}\n",
      "]\n",
      "}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "langauge_system_prompt = \"\"\" \n",
    "你是一家软件公司的会议助理。会议记录以音频、PowerPoint或PDF形式提供。\n",
    "你的一般职责包括但不限于总结会议中的重要要点，记录每个人的交付物，并以清晰简洁的形式组织所有信息。\n",
    "阅读你的输出后，所有成员都应清楚了解会议的目标、议程和结果。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_prompt = \"\"\" \n",
    "输出为中文\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
