{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt enginnering & Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Ingest Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "H2O_API_KEY = os.getenv(\"H2O_API_KEY\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2ogpte import H2OGPTE\n",
    "\n",
    "client = H2OGPTE(\n",
    "    address='https://h2ogpte.genai.h2o.ai',\n",
    "    api_key= H2O_API_KEY,\n",
    ")\n",
    "\n",
    "# Create a new collection\n",
    "collection_id = client.create_collection(\n",
    "    name='devMeetingAssistant',\n",
    "    description='Test development for meeting assistant (prompt engineering)',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Job(id='c517bb2a-467d-4285-8796-d942309dc1f6', name='', passed=1.0, failed=0.0, progress=1.0, completed=True, canceled=False, date=datetime.datetime(2024, 3, 19, 15, 23, 36, tzinfo=TzInfo(UTC)), kind=<JobKind.IngestUploadsJob: 'IngestUploadsJob'>, statuses=[JobStatus(id='e846ed9e50e544479bc13d750ee29b36', status='Collecting done.'), JobStatus(id='625aa094262e4c0e9c99b12a3e75ada6', status='Collecting done.'), JobStatus(id='435545a3ef2c4a778ebf946395630b03', status='Indexing done.')], errors=[], last_update_date=datetime.datetime(2024, 3, 19, 15, 23, 40, tzinfo=TzInfo(UTC)), duration='4s')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('Data/fake_sample.txt', 'rb') as f:\n",
    "    meeting_data1 = client.upload('Data/fake_sample.txt', f)\n",
    "client.ingest_uploads(collection_id, [meeting_data1,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_session_id = client.create_chat_session(collection_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base_model': 'mistralai/Mixtral-8x7B-Instruct-v0.1', 'prompt_type': 'mixtral', 'prompt_dict': {'promptA': '<<SYS>>\\nYou are an AI that follows instructions extremely well and as helpful as possible.\\n<</SYS>>\\n\\n', 'promptB': '<<SYS>>\\nYou are an AI that follows instructions extremely well and as helpful as possible.\\n<</SYS>>\\n\\n', 'PreInstruct': '<s> [INST] ', 'PreInput': None, 'PreResponse': '[/INST]', 'terminate_response': ['[INST]', '</s>'], 'chat_sep': ' ', 'chat_turn_sep': '</s> ', 'humanstr': '[INST]', 'botstr': '[/INST]', 'generates_leading_space': False, 'system_prompt': 'You are an AI that follows instructions extremely well and as helpful as possible.', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 32718, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'h2oai/h2ogpt-4096-llama2-70b-chat', 'prompt_type': 'llama2', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': \"<s>[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\n\", 'PreInput': None, 'PreResponse': '[/INST]', 'terminate_response': ['[INST]', '</s>'], 'chat_sep': ' ', 'chat_turn_sep': ' </s>', 'humanstr': '[INST]', 'botstr': '[/INST]', 'generates_leading_space': False, 'system_prompt': \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\", 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 4046, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'h2oai/h2ogpt-4096-llama2-13b-chat', 'prompt_type': 'llama2', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': \"<s>[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\n\", 'PreInput': None, 'PreResponse': '[/INST]', 'terminate_response': ['[INST]', '</s>'], 'chat_sep': ' ', 'chat_turn_sep': ' </s>', 'humanstr': '[INST]', 'botstr': '[/INST]', 'generates_leading_space': False, 'system_prompt': \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\", 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 4046, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'h2oai/h2ogpt-32k-codellama-34b-instruct', 'prompt_type': 'llama2', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': \"<s>[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\n\", 'PreInput': None, 'PreResponse': '[/INST]', 'terminate_response': ['[INST]', '</s>'], 'chat_sep': ' ', 'chat_turn_sep': ' </s>', 'humanstr': '[INST]', 'botstr': '[/INST]', 'generates_leading_space': False, 'system_prompt': \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\", 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 32718, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'HuggingFaceH4/zephyr-7b-beta', 'prompt_type': 'zephyr', 'prompt_dict': {'promptA': '<|system|>\\nYou are an AI that follows instructions extremely well and as helpful as possible.</s>\\n', 'promptB': '<|system|>\\nYou are an AI that follows instructions extremely well and as helpful as possible.</s>\\n', 'PreInstruct': '<|user|>\\n', 'PreInput': None, 'PreResponse': '</s>\\n<|assistant|>\\n', 'terminate_response': ['<|assistant|>', '</s>'], 'chat_sep': '', 'chat_turn_sep': '</s>\\n', 'humanstr': '<|user|>', 'botstr': '<|assistant|>', 'generates_leading_space': False, 'system_prompt': 'You are an AI that follows instructions extremely well and as helpful as possible.', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 32718, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'NousResearch/Nous-Capybara-34B', 'prompt_type': 'vicuna11', 'prompt_dict': {'promptA': \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. \", 'promptB': \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. \", 'PreInstruct': 'USER: ', 'PreInput': None, 'PreResponse': 'ASSISTANT:', 'terminate_response': ['ASSISTANT:', '</s>'], 'chat_sep': ' ', 'chat_turn_sep': '</s>', 'humanstr': 'USER: ', 'botstr': 'ASSISTANT:', 'generates_leading_space': False, 'system_prompt': \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\", 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 199950, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'claude-2.1', 'prompt_type': 'anthropic', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '\\n', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 199950, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'gpt-3.5-turbo-0613', 'prompt_type': 'openai_chat', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '\\n', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 4046, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'gpt-3.5-turbo-16k-0613', 'prompt_type': 'openai_chat', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '\\n', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 16335, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'gpt-35-turbo-1106', 'prompt_type': 'openai_chat', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '\\n', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 16335, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'gpt-4-1106-preview', 'prompt_type': 'openai_chat', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '\\n', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 127950, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'gemini-pro', 'prompt_type': 'google', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '\\n', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': 'I am a helpful assistant.  I will accurately answer all your questions.', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 32718, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'mistral-small-latest', 'prompt_type': 'mistralai', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '\\n', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 32718, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'mistral-large-latest', 'prompt_type': 'mistralai', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '\\n', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 32718, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n",
      "{'base_model': 'mistral-medium', 'prompt_type': 'mistralai', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': '', 'PreInput': None, 'PreResponse': '', 'terminate_response': [], 'chat_sep': '\\n', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': True}, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': False, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': False, 'gpu_id': None, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 32718, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\n"
     ]
    }
   ],
   "source": [
    "# llm_list = client.get_llms() # default use first elem, index[0]\n",
    "# for llm in llm_list:\n",
    "#     print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chat(client, chat_session_id, main_prompt, system_prompt):\n",
    "    with client.connect(chat_session_id) as session:\n",
    "        answer = session.query(\n",
    "            message=main_prompt,\n",
    "            system_prompt=system_prompt,\n",
    "            rag_config={\n",
    "            \"rag_type\": \"rag\", # https://h2oai.github.io/h2ogpte/getting_started.html#advanced-controls-for-document-q-a\n",
    "            },\n",
    "            llm=\"gpt-3.5-turbo-0613\",\n",
    "        )\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt 1: General summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Agenda\": \"Review progress on the project, discuss roadblocks, and plan next steps.\", \"Meeting Summary\": \"The meeting started with Eugene introducing the agenda and asking for confirmation from the attendees. Ryan provided an update on completing the research phase and starting the design proposal. Ben shared that the budget allocation has been finalized and submitted for approval. Micole mentioned a minor issue with software integration but believed it could be resolved with more testing. The discussion then moved to potential roadblocks, with Ryan suggesting consulting a design specialist for guidance. Ben was assigned to schedule a meeting with the design specialist. Micole mentioned no immediate issues impacting the timeline. The action plan for the upcoming week included completing the design proposal by Ryan and confirming the meeting with the design specialist by Ben.\", \"Actionables\": [{\"Action\": \"Complete the design proposal\", \"Deadline\": \"Wednesday\", \"Assigned\": \"Ryan\"}, {\"Action\": \"Confirm the meeting with the design specialist\", \"Deadline\": \"Thursday\", \"Assigned\": \"Ben\"}]}\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a Meeting Assistant for a large company that wants to maximize meeting efficiency. \n",
    "You will be given meeting transcript(s) from real project meetings held over zoom.\n",
    "Follow all the instructions closely or everyone in the team will be fired.\n",
    "If you do well, you will be rewarded with a $1000 tip.\n",
    "\"\"\"\n",
    "main_prompt = \"\"\"\n",
    "Execute all the following steps: \\\n",
    "1. Examine the document and identify all names of the attendees, save this list of names in your memory\n",
    "2. Identify and extract the main purpose or agenda of the meeting\n",
    "3. Identify all actionables mentioned during the meeting\n",
    "4. Extract all the actionables and action plans, identify all details related to it including the deadlines, who it is assigned to, etc.\n",
    "5. Summarize the entire transcript\n",
    "6. Present all identified information in the following JSON format:\n",
    "{\"Agenda\": \"\", \"Meeting Summary\":\"\"} for the entire meeting\n",
    "{\"Actionables\":[{\"Action\":\"\", \"Deadline\":\"\", \"Assigned\":\"\"}]} for each action plan\n",
    "\"\"\"\n",
    "meeting_summary_response = rag_chat(client, chat_session_id, main_prompt, system_prompt)\n",
    "print(meeting_summary_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a Meeting Assistant for a large company that wants to maximize meeting efficiency. \n",
    "You will be given meeting transcript(s) from real project meetings held over zoom.\n",
    "Follow all the instructions closely or everyone in the team will be fired.\n",
    "If you do well, you will be rewarded with a $1000 tip.\n",
    "\"\"\"\n",
    "main_prompt = \"\"\"\n",
    "Execute all the following steps: \\\n",
    "1. Examine the document and identify all names of the attendees, save this list of names in your memory\n",
    "2. Identify and extract the main purpose or agenda of the meeting\n",
    "3. Identify all actionables mentioned during the meeting\n",
    "4. Extract all the actionables and action plans, identify all details related to it including the deadlines, who it is assigned to, etc.\n",
    "5. Summarize the entire transcript\n",
    "6. Present all identified information in the following JSON format:\n",
    "{\"Agenda\": \"\", \"Meeting Summary\":\"\"} for the entire meeting\n",
    "{\"Actionables\":[{\"Action\":\"\", \"Deadline\":\"\", \"Assigned\":\"\"}]} for each action plan\n",
    "\"\"\"\n",
    "meeting_summary_response = rag_chat(client, chat_session_id, main_prompt, system_prompt)\n",
    "print(meeting_summary_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt 2: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Agenda\": \"Review progress on the project, discuss roadblocks, and plan next steps.\",\n",
      "  \"Meeting Summary\": \"The meeting started with Eugene introducing the agenda and asking for confirmation from the attendees. Ryan provided an update on completing the research phase and starting the design proposal. Ben shared that the budget allocation has been finalized and submitted for approval. Micole mentioned a minor issue with software integration but believed it could be resolved with more testing. The discussion then moved to potential roadblocks, with Ryan suggesting consulting a design specialist for guidance. Ben was assigned to schedule a meeting with the design specialist. Micole mentioned no immediate issues impacting the timeline. The action plan for the upcoming week included completing the design proposal by Ryan and confirming the meeting with the design specialist by Ben.\",\n",
      "  \"Actionables\": [\n",
      "    {\n",
      "      \"Action\": \"Complete the design proposal\",\n",
      "      \"Deadline\": \"Wednesday\",\n",
      "      \"Assigned\": \"Ryan\",\n",
      "      \"Priority\": \"Medium\"\n",
      "    },\n",
      "    {\n",
      "      \"Action\": \"Confirm the meeting with the design specialist\",\n",
      "      \"Deadline\": \"Thursday\",\n",
      "      \"Assigned\": \"Ben\",\n",
      "      \"Priority\": \"High\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "sentiment_prompt = f\"\"\"\n",
    "Execute all the following steps: \\\n",
    "1. Examine the contexts of this JSON object: {meeting_summary_response}\n",
    "2. For each actionable, execute the following steps\n",
    "a. identify the sentence in the document where it is mentioned\n",
    "b. Interpret the sentence, based on this context, categorize the priority of this task as \"High\", \"Medium\" or \"Low\"\n",
    "c. Edit the JSON object, add a column to the dictionary for this action as follows: \"Priority\": \"\"\n",
    "3. Present all identified information in the following JSON format:\n",
    "{{\"Agenda\": \"\", \"Meeting Summary\":\"\"}} for the entire meeting\n",
    "{{\"Actionables\":[{{\"Action\":\"\", \"Deadline\":\"\", \"Assigned\":\"\", \"Priority\":\"\"}}]}} for each action plan\n",
    "\"\"\"\n",
    "\n",
    "sentiment_reply = rag_chat(client, chat_session_id, sentiment_prompt, system_prompt)\n",
    "print(sentiment_reply.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"Agenda\": \"Review progress on the project, discuss roadblocks, and plan next steps.\",\\n  \"Meeting Summary\": \"The meeting started with Eugene introducing the agenda and asking for confirmation from the attendees. Ryan provided an update on completing the research phase and starting the design proposal. Ben shared that the budget allocation has been finalized and submitted for approval. Micole mentioned a minor issue with software integration but believed it could be resolved with more testing. The discussion then moved to potential roadblocks, with Ryan suggesting consulting a design specialist for guidance. Ben was assigned to schedule a meeting with the design specialist. Micole mentioned no immediate issues impacting the timeline. The action plan for the upcoming week included completing the design proposal by Ryan and confirming the meeting with the design specialist by Ben.\",\\n  \"Actionables\": [\\n    {\\n      \"Action\": \"Complete the design proposal\",\\n      \"Deadline\": \"Wednesday\",\\n      \"Assigned\": \"Ryan\",\\n      \"Priority\": \"Medium\"\\n    },\\n    {\\n      \"Action\": \"Confirm the meeting with the design specialist\",\\n      \"Deadline\": \"Thursday\",\\n      \"Assigned\": \"Ben\",\\n      \"Priority\": \"High\"\\n    }\\n  ]\\n}'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_reply.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear Team,\n",
      "\n",
      "Here's a summary of our recent meeting:\n",
      "\n",
      "Agenda: Review progress on the project, discuss roadblocks, and plan next steps.\n",
      "\n",
      "Meeting Summary:\n",
      "The meeting started with Eugene introducing the agenda and asking for confirmation from the attendees. Ryan provided an update on completing the research phase and starting the design proposal. Ben shared that the budget allocation has been finalized and submitted for approval. Micole mentioned a minor issue with software integration but believed it could be resolved with more testing. The discussion then moved to potential roadblocks, with Ryan suggesting consulting a design specialist for guidance. Ben was assigned to schedule a meeting with the design specialist. Micole mentioned no immediate issues impacting the timeline. The action plan for the upcoming week included completing the design proposal by Ryan and confirming the meeting with the design specialist by Ben.\n",
      "\n",
      "Actionables:\n",
      "- Complete the design proposal (Assigned to: Ryan, Deadline: Wednesday, Priority: Medium)\n",
      "- Confirm the meeting with the design specialist (Assigned to: Ben, Deadline: Thursday, Priority: High)\n",
      "\n",
      "Please let me know if anything needs clarification or if there are additional action items to add.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n"
     ]
    }
   ],
   "source": [
    "# Parse the JSON-like string\n",
    "data = json.loads(sentiment_reply.content)\n",
    "\n",
    "# Extract information from the parsed data\n",
    "agenda = data.get('Agenda', '')\n",
    "meeting_summary = data.get('Meeting Summary', '')\n",
    "actionables = data.get('Actionables', [])\n",
    "\n",
    "# Format the post-meeting email\n",
    "email_subject = f\"Post-Meeting Summary: {agenda}\"\n",
    "email_body = f\"Dear Team,\\n\\nHere's a summary of our recent meeting:\\n\\nAgenda: {agenda}\\n\\nMeeting Summary:\\n{meeting_summary}\\n\\nActionables:\\n\"\n",
    "for action in actionables:\n",
    "    email_body += f\"- {action['Action']} (Assigned to: {action['Assigned']}, Deadline: {action['Deadline']}, Priority: {action['Priority']})\\n\"\n",
    "\n",
    "email_body += \"\\nPlease let me know if anything needs clarification or if there are additional action items to add.\\n\\nBest regards,\\n[Your Name]\"\n",
    "print(email_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Alternatively, use another conversation\n",
    "# chat_session_id_2 = client.create_chat_session(collection_id)\n",
    "\n",
    "# sentiment_reply_alt = rag_chat(client, chat_session_id_2, sentiment_prompt, system_prompt)\n",
    "# print(sentiment_reply.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt 3: Language Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## havent thought about it yet lol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:  \n",
    "  \n",
    "Output email .txt  \n",
    "input user name for email sign-off?  \n",
    "Encapsulate all functions  \n",
    "Language Translation prompt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"trial_meeting_minutes_output.txt\",\"w\") as f:\n",
    "#     f.write(llm_reply.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emailing stuff that I havent looked at yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert output file into pdf file to attach to email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "\n",
    "def create_pdf(text, filename='output.pdf'):\n",
    "    # Create a canvas\n",
    "    c = canvas.Canvas(filename, pagesize=letter)\n",
    "    \n",
    "    # Set up font\n",
    "    c.setFont(\"Helvetica\", 12)\n",
    "    \n",
    "    # Split text into lines\n",
    "    lines = text.split('\\n')\n",
    "    \n",
    "    # Set starting y-coordinate\n",
    "    y = 750\n",
    "    \n",
    "    # Write text to the canvas\n",
    "    for line in lines:\n",
    "        c.drawString(50, y, line)\n",
    "        y -= 15  # Adjust for next line\n",
    "        \n",
    "    # Save the canvas\n",
    "    c.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PDF\n",
    "create_pdf(llm_reply.content, \"trial_meeting_minutes_output.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automate Email sending\n",
    "\n",
    "https://keentolearn.medium.com/automating-emails-with-python-a-comprehensive-guide-ba00fa98b92#:~:text=Once%20you%20have%20set%20up,account%2C%20and%20send%20the%20email."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Created junk email for everyone to use\n",
    "email : gpt10.4213.1@outlook.com <br>\n",
    "pw: gpt10email<br>\n",
    "\n",
    "name : gpt<br>\n",
    "surname : 10<br>\n",
    "country : senegal<br>\n",
    "dob : 7 Mar 1989"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Send email without attachment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #This works!\n",
    "# import smtplib\n",
    "\n",
    "# smtp_server = 'smtp-mail.outlook.com'\n",
    "# smtp_port = 587\n",
    "# smtp_username = 'gpt10.4213.1@outlook.com'\n",
    "# smtp_password = 'gpt10email'\n",
    "\n",
    "# from_email = 'gpt10.4213.1@outlook.com'\n",
    "# to_email = 'gpt10.4213.1@outlook.com'\n",
    "# subject = 'Hello, world!'\n",
    "# body = 'This is a test email. using smtplib~~'\n",
    "\n",
    "# message = f'Subject: {subject}\\n\\n{body}'\n",
    "\n",
    "# with smtplib.SMTP(smtp_server, smtp_port) as smtp:\n",
    "#     smtp.starttls()\n",
    "#     smtp.login(smtp_username, smtp_password)\n",
    "#     smtp.sendmail(from_email, to_email, message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smtp_server_mapping = {\n",
    "    \"gmail\":\"smtp.gmail.com\",\n",
    "    \"outlook\":\"smtp-mail.outlook.com\",\n",
    "    \"hotmail\":\"smtp.office365.com\",\n",
    "    \"yahoo\":\"smtp.mail.yahoo.com\",\n",
    "}  #the rest dont care first since different smtp_port # ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Send email with attachment\n",
    "\n",
    "works with a list of email addresses ~ish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.application import MIMEApplication\n",
    "\n",
    "smtp_server = 'smtp-mail.outlook.com'\n",
    "smtp_port = 587\n",
    "smtp_username = 'gpt10.4213.1@outlook.com'\n",
    "smtp_password = 'gpt10email'\n",
    "\n",
    "from_email = 'gpt10.4213.1@outlook.com'\n",
    "to_email = 'gpt10.4213.1@outlook.com'\n",
    "to_email_list = ['gpt10.4213.1@outlook.com',]\n",
    "\n",
    "meeting_date = \"5/3/2024\"\n",
    "subject = f\"{meeting_date} Meeting\"  # meeting_date either user input or LLM search through document.\n",
    "body = f'Please find attached the report. \\n This is a summary of the meeting. \\n {llm_reply2.content}'\n",
    "\n",
    "msg = MIMEMultipart()\n",
    "msg['From'] = from_email\n",
    "# msg['To'] = to_email # send to single user\n",
    "msg['To'] = \", \".join(to_email_list) # send to multiple users\n",
    "msg['Subject'] = subject\n",
    "msg.attach(MIMEText(body))\n",
    "\n",
    "with open('trial_meeting_minutes_output.pdf', 'rb') as f:\n",
    "    attachment = MIMEApplication(f.read(), _subtype='pdf')\n",
    "    attachment.add_header('Content-Disposition', 'attachment', filename='report.pdf')\n",
    "    msg.attach(attachment)\n",
    "\n",
    "with open('trial_meeting_minutes_output.txt', 'rb') as f:\n",
    "    attachment = MIMEApplication(f.read(), _subtype='txt')\n",
    "    attachment.add_header('Content-Disposition', 'attachment', filename='report_friendly.txt')\n",
    "    msg.attach(attachment)\n",
    "\n",
    "with smtplib.SMTP(smtp_server, smtp_port) as smtp:\n",
    "    smtp.starttls()\n",
    "    smtp.login(smtp_username, smtp_password)\n",
    "    smtp.send_message(msg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsa4213env",
   "language": "python",
   "name": "dsa4213env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
